{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from IPython.core.display import HTML\n",
    "def generate_path(path):\n",
    "    folder = os.path.exists(path)\n",
    "    if not folder:\n",
    "        os.makedirs(path)\n",
    "def video(video_url,file_name):\n",
    "    return urllib.request.urlretrieve(video_url,file_name)\n",
    "def cover(cover_url,file_name):\n",
    "    return urllib.request.urlretrieve(cover_url,file_name)\n",
    "def time(timestamp):\n",
    "    return str(datetime.datetime.fromtimestamp(timestamp))\n",
    "api = requests.get('https://raw.githubusercontent.com/xjincomm/Douyin/master/Douyin%20Trending%20API.txt').text\n",
    "re=requests.get(api)\n",
    "soup=bs.BeautifulSoup(re.content,'html.parser')\n",
    "last_update = json.loads(soup.text)['data']['active_time']\n",
    "trending_data = json.loads(soup.text)['data']['word_list']\n",
    "trend = pd.DataFrame(trending_data)\n",
    "date=last_update.split(' ')[0]\n",
    "Word_cover=[]\n",
    "for i in trend['word_cover']:\n",
    "    if type(i)==dict:\n",
    "        Word_cover.append(i['url_list'][0])\n",
    "    else:\n",
    "        Word_cover.append(None)\n",
    "trend = trend.drop(columns = ['word_cover','challenge_id'])\n",
    "# word means the title of the topic\n",
    "generate_path('./trend')\n",
    "#trend['cover']=['<img src=\"'+ str(i) + '\" width=\"60\" >' for i in Word_cover]\n",
    "#trend_visual = HTML(trend.head(3).to_html(escape=False ,formatters=trend['cover']))\n",
    "#trend.to_html('./trend/trend_'+last_update+'.html', escape=False)\n",
    "trend.to_csv('./trend/trend_'+date+'.csv',encoding = 'utf-8-sig', index = False)\n",
    "#trend_visual\n",
    "def scraper(topic):\n",
    "    generate_path('./'+topic)\n",
    "    topic_api='https://aweme-hl.snssdk.com/aweme/v1/hot/search/video/list/?hotword='\n",
    "    re=requests.get(topic_api+topic)\n",
    "    soup=bs.BeautifulSoup(re.content,'html.parser')\n",
    "    data = json.loads(soup.text)\n",
    "    data = data['aweme_list']\n",
    "    desc = [info['desc'] for info in data]\n",
    "    time_stamp = [info['create_time'] for info in data]\n",
    "    create_time = [time(info['create_time']) for info in data]\n",
    "    nickname = [info['author']['nickname'] for info in data]\n",
    "    verify = [info['author']['custom_verify'] for info in data]\n",
    "    share_count = [info['statistics']['share_count'] for info in data]\n",
    "    forward_count = [info['statistics']['forward_count'] for info in data]\n",
    "    like_count = [info['statistics']['digg_count'] for info in data]\n",
    "    comment_count = [info['statistics']['comment_count'] for info in data]\n",
    "    download_count = [info['statistics']['download_count'] for info in data]\n",
    "    cover_url = [info['video']['cover']['url_list'][0] for info in data]\n",
    "    cover_visual = ['<img src=\"'+ url + '\" width=\"100\" >' for url in cover_url]\n",
    "    video_url = []\n",
    "    for info in data:\n",
    "        try:\n",
    "            video_url.append([i for i in info['video']['download_addr']['url_list'] if 'default' in i][0])\n",
    "        except:\n",
    "            video_url.append(None)\n",
    "    df=pd.DataFrame({'desc':desc,'nickname':nickname,'verify':verify,'time_stamp':time_stamp,\n",
    "                     'create_time':create_time,'share_count':share_count,'forward_count':forward_count,\n",
    "                    'like_count':like_count,'comment_count':comment_count,\n",
    "                     'download_count':download_count,'video_url':video_url,\n",
    "                    'cover_visual':cover_visual})\n",
    "    df.to_csv('./'+topic+'/'+topic+'.csv',encoding='utf-8-sig',index=False)\n",
    "    #df.to_html('./'+topic+'/'+topic+'.html',escape=False)\n",
    "    #video_visual = HTML(df.to_html(escape=False ,formatters=df['cover_visual']))\n",
    "    for num in range(0,len(data)):\n",
    "        try:\n",
    "            video(df['video_url'][num],'./'+topic+'/'+str(df['time_stamp'][num])+'.mp4')\n",
    "            \n",
    "            print('topic: '+topic+', video #'+str(num)+': '+str(df['time_stamp'][num])+'......Successed')\n",
    "        except:\n",
    "            print('topic: '+topic+', video #'+str(num)+': '+str(df['time_stamp'][num])+'......Failed')\n",
    "            continue\n",
    "def douyin_trend():\n",
    "    for i in trend['word']:\n",
    "        scraper(i)\n",
    "def douyin_topic(topic):\n",
    "    scraper(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
